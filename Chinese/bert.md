### BERT：基于语义理解的深度双向转换器(Transformer)的预训练模型



#### 摘要

​		本文我们将介绍一个新的语义表示模型：BERT，全称为**基于转换器模型的双向编码表示**。不同于其他语义表示模型，BERT在每一层中，通过对无标注文本的上下文预训练来学习其深度双向表示。因此，预训练的BERT模型可以仅通过添加额外的输入层进行微调即可适应不同的任务场景，建立与之相关的模型，如问答模型与语义推理，而不需要针对任务进行模型结构的调整。

​		BERT是一个简单却实用的模型。它在11个自然语言处理任务中打破了分数记录，成绩如下表：

|              | GLUE  | MultiNLI准确度 | SQuAD v1.1 F1 值 | SQuAD v2.0 F1 值 |
| :----------: | :---: | :------------: | :--------------: | :--------------: |
|   BERT分值   | 80.5% |     86.7%      |       93.2       |       83.1       |
| 较以往提升值 | 7.7%  |      4.6%      |       1.5        |       5.1        |



####  1 导论

已有经验表明语言模型的预训练可以有效提升许多自然语言处理（NLP）任务模型的效果，包括通过整句建模表达句与句之间关系的**语句级**任务，如语义推理，语句改写等；也包括输出更细粒度的**字词级**结果的任务，如命名实体识别与问答模型。

> ​		预训练语义表示模型在下游NLP任务中的应用可分为两种：**基于特征**与**基于微调**。
>
> * 基于特征的方式，如ELMo，将预训练表示作为额外的特征输入参与到*任务特异的模型*（*根据任务场景不同有针对性地设计模型架构*）训练中。
>
> * 基于微调的方式，如生成式预训练转换器模型（OpenAI GPT），希望尽可能少地引入任务特异的参数，而不同任务的训练过程仅仅是靠对*所有*预训练参数的微调进行。
>
>   这两种方式在预训练过程中均采取了**单向语言模型**的模式来构造目标方程，从而实现语义表示。
>

我们认为现有的技术不能完全发挥出预训练表示法的能力，尤其是采用微调的方式的时候。主要限制在于标准语言模型的单向性思维会影响到预训练模型的结构设计。例如，在OpenAI GPT模型中，作者采用了从左至右的模型结构，致使在转换器模块中，所有自注意层中的每个单词只能“注意”到在它之前的单词。这种限制造成语句级任务结果表现次优，而且非常不利于采用微调方式训练的类似问答模型的字词级任务模型优化，因为这类任务的解决极大依赖于上下文的双向表达。

​		在本文中，我们提出BERT模型以改进采用微调方式的预训练模型应用。BERT中使用“被遮盖的语言模型”（**MLM**）作为预训练的目标，从而摆脱了之前提到的单向限制。这个思路参考自完形填空任务的设计模式（Taylor, 1953）。MLM随机遮盖住一部分输入的单词，基于对这些被遮盖的单词的上下文分析，输出被遮盖单词的对应 id。不同于从左至右的预训练语言模型，MLM的目标设定使得模型能融合左右两边的内容实现表示，从而让深度双向转换器的预训练成为可能。除了MLM，我们还设计了“预测是否下一句”（**NSP**）的任务来同时训练得到文本对的表示。本文作出的贡献如下：

* 我们展示了双向语义表示预训练的重要性。不同于Radford等人（2018）设计的预训练模型，BERT通过MLM实现了语义的双向表示。也不同于Peters等人（2018a）的模型，他们仅仅通过分别独立训练的从左至右与从右至左的两个LM模型结果的简单拼接得到表示，本质上也是基于单向的表示法。
* 我们证明预训练模型适用于许多以往需要特殊复杂模型设计的任务场景。BERT是首个通过微调在大量语句级以及字词级NLP任务中达到最优表现，亦优于许多任务特异的模型架构。
* BERT刷新了11个NLP任务的最优记录。代码和预训练模型可以在https://github.com/google-research/bert中获得。



#### 2 相关工作

模型预训练的思想有很久的历史了。在这里我们简单回顾下使用最广泛的预训练方法。

##### 2.1 基于特征的非监督学习方法

近几十年来，学习能够广泛应用于不同场景的的词向量表示是NLP中一个非常活跃的领域，包括非神经网络与神经网络模型。使用预训练的词嵌入是现代NLP领域的通用方法，相比于从零训练词的嵌入表示而言，预训练能极大提升NLP模型的表现。在词嵌入向量的预训练模型中，一般采用从左至右的语言建模方式，目标设定为基于上文（左边）的内容预测下一个词。

​		这些方法同样被应用到更粗粒度的语言建模中，如句嵌入或者段落的嵌入表示。在句向量的表示方面，有的研究采取预测下一句的作为目标，依次生成单词的下一句生成过程由上一句的向量表示激活，或者目标函数由自动编码器的降噪过程导出。

​		ELMo及它的变种从另一个角度进行传统词向量的研究。它们分别从两个方向提取上下文相关的信息：从左至右和从右至左，而最终生成的上下文表示是这两者分别得到的表示的拼接。通过将拼接得到的向量作为不同任务模型的额外输入，ELMo在好几个主流NLP基准模型（包括问答，情感分析与命名实体识别）中表现出色，打破了原有最优记录。Melamud等人（2016）提出通过LSTM从左右两个方向实现上下文表示，这与ELMo类似，都是基于特征且仅实现浅层的双向表示。Fedus等人（2018）证明参考完形填空任务的模式来设计文本生成模型可以提高模型的稳健度。

##### 2.2 基于微调的非监督学习方法

就像基于特征的方法一样，这一方向最先的研究思路是仅从未标注的文本中预训练得到词的嵌入表示参数（而不包括其他参数）。

​		而更近的研究关注于从未标注文本中预训练得到整个句子或段落的编码器后——该编码器能生成词的上下文表示——再通过微调的方式实现一个下游的监督学习任务模型。这样做的优势在于对于每一个新的下游任务，需要添加的的待训练参数很少。OpenAI GPT之所以能够在很多GLUE基准的语句级任务中复现历史最优效果，至少有部分是源于基于微调的这样一个设计。从左至右的语言模型与自动编码的目标设计应用在了这类预训练模型中。

##### 2.3 基于标注数据的转移学习

同样也有研究表明基于大数据集训练的监督模型能够实现很好的转移效果，比如说自然语言推理与机器翻译。计算机视觉领域的研究也表明大型预训练模型在转移学习中的重要性，一个非常有效的方法即用ImageNet数据集做预训练，然后根据特定任务进行微调。



#### 3 BERT

这部分我们将介绍BERT模型与它的具体应用。我们的工作可分为两步：**预训练**和**微调**。

* 在预训练过程中，模型的训练数据是来自于所有预训练任务的未标注数据集。

* 在微调过程中，BERT首先用预训练得到的参数进行初始化，然后再基于不同下游任务的标注数据进行微调，微调在所有参数中进行。尽管共享初始化预训练参数，不同下游任务的具体微调模型是不同的。

  详情可参考图1的问答模型范例。

  ![image-20200706153516831](/Users/liufen/Library/Application Support/typora-user-images/image-20200706153516831.png)

  [^图1]: BERT预训练与微调的整个过程。除了输出层外，预训练与微调模型共享同样的模型结构。不同的下游任务使用同样的预训练参数做初始化。在微调过程中，所有的参数都会被微调。在每个输入样本前都会增加一个[CLS]标记，此外，还有[SEP]用于标记分隔符，如问句与答句之间的分隔。

​		BERT的一个特征是在不同任务中它的结构是高度统一的。预训练模型与最终的下游任务模型在结构上仅有少许的不同（即输出部分）。

**模型结构**	BERT的模型结构是一个多层的双向转换器编码部分，转换器模型最初由Vaswani等人（2017）提出并在tensor2tensor包中发布了它的代码。因为转换器模型是现在非常常用的一个模型，而我们的模型几乎与最原始的转换器模型一致，所以在此我们对这个模型结构的详细背景不作过多说明，想要了解这块的话可以去读Vaswani等人的那篇论文，同样我们也推荐一篇优秀的导读文章“[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)”。

​		在本文中，我们将模型的层数（即转换器区块数目）用 L 表示， 隐层大小用 H 表示，自注意力机制的头数用 A 表示。我们主要展示两个不同尺寸模型的结果：$BERT_{BASE}$（L=12，H=768，A=12，总参数量=110M）与 $BERT_{LARGE}$（L=24，H=1024，A=16，总参数量=340M）。

​		$BERT_{BASE}$ 是为了与OpenAI GPT做比较而选择与其相同的模型尺寸。然而，严格来说，BERT的转换器使用了双向自注意机制（我们称之为转换器的编码部分），而GPT的转换器使用了受限的自注意机制，每个单词仅能注意到它左边的内容（我们称之为转换器的解码部分，能实现文本生成）。

**输入/输出表示**  为了增加BERT的适用范围，我们的模型输入设计能够同时满足单句与语句对的输入形式（如问答对），语句对是拼接成一条单词序列进行输入的。在下文中，<u>一个“句子”可能是一串任意连续文本，而并不限制为一个具有实际语言意义的语句。一个“序列”指的是BERT的输入单词序列，它可以是单句也可以是两个句子的拼接。</u>

​		我们采用WordPiece嵌入，它有了30,000个词汇量。每个序列总是以一个特殊的词（[CLS]）开头，这个词最后的隐层状态对应着整个序列的一个综合表示，该表示可用于分类任务模型。语句对则被打包成一条序列，这两个句子可以从两个方面进行区分。首先，我们用 [SEP] 将它们间隔开。其次，我们对每个单词增加了一个额外的嵌入用来说明该单词是数据句子 A 还是句子 B 。如图1 所示，我们将输入的嵌入用 E 表示，特殊词 [CLS] 最后的隐层向量用$C\in\R^H$表示，而$i^{th}$输入的最后隐层向量表示为$T_i\in\R^H$。

​		对于一个给定的单词，它的输入表示是由它对应的单词嵌入，语句区分嵌入，以及位置嵌入三者加总得到。详情请看图2。

![image-20200706192149258](/Users/liufen/Library/Application Support/typora-user-images/image-20200706192149258.png)

[^图2]: BERT的输入表示。由各自对应的单词嵌入，语句区分嵌入，以及位置嵌入三者加总得到。

##### 3.1 BERT预训练

不同于Peters等人（2018a）和Radford等人（2018）的研究，我们不使用传统的从左至右或者从右至左的语言模型来预训练BERT。与之相对的，我们将在这个部分介绍两个用来预训练BERT的无监督任务。这一步可见图1 的左边部分。

**任务1: 被遮盖的语言模型（MLM）**  直观上来说，一个深层双向的模型明显比单向模型或者两个单向模型的简单拼接更有语义代表性。不幸的是，传统的语言模型只能单向训练，因为双向的训练，在多层注意的作用下，容易导致每个单词间接地“注意到自己”，进而使得预测无意义。

​		为了训练一个深层双向表示模型，我们只是简单地随机遮盖住了输入序列中的一部分单词，然后预测这部分被遮盖的单词。这个过程我们称之为“被遮盖的语言模型”（MLM），尽管在文学中它被称作完形填空。在这个设定下，被遮盖住的单词对应的最后隐层可后接在整个字典范围进行softmax的输出层，就像在标准语言模型中做的那样。在实验过程中，我们为每个输入序列随机遮盖住15%的WordPiece词汇，与降噪的自编码器不同的是，我们只预测被遮盖住的单词而不是重构整个输入。

​		尽管这让我们获得了一个双向的预训练模型，但有一个缺点是造成了预训练与微调过程的不匹配，因为微调过程中不会出现单词 [MASK]。为了减轻这个不匹配带来的影响，我们并不总是将被掩盖的单词替换成 [MASK]。也就是说，训练的时候我们随机选择15%的输入单词位置进行预测。如果第 i 个单词被选中了，我们将第 i 个单词进行以下处理：

（1）80%的概率被替换成 [MASK]；

（2）10%的概率被替换成一个随机的单词；

（3）10%的概率不被替换。

然后，$T_i$会被用来通过计算交叉熵损失预测原始的单词。在附录C.2中我们比较了不同替换概率的组合产生的结果。

**任务2: 下一句预测模型（NSP）**  很多下游NLP任务需要基于对两个句子之间的关系的理解进行建模，比如说问答（QA）和自然语言推理（NLI），而这种关系在语言模型中无法直接抓取。为了得到一个能理解句子之间关系的模型，我们在预训练过程中设计了一个预测是否下一句的二分类任务。这个任务仅需要任一语言的语料库即可训练。更确切地说，假如我们选择句子A和句子B作为某训练样本，B有50%的概率是A的下一句（标记为 IsNext）, 50%的概率是从语料库中随机抽取的一个句子（标记为 NotNext）。如图1 所见，C 用来预测是否是下一句。尽管这是一个很简单的设计，我们发现NSP的训练非常有助于QA和NLI的建模表现（详见5.1部分）。

​		NSP的设定与Jernite等人（2017）与Logeswaran和Lee（2018）的设定很像，但是他们只将句向量迁移到了下游任务中，而BERT将所有的参数作为下游模型的初始化参数进行微调训练。

**预训练数据**  预训练过程很大程度上参考了现存语言模型预训练研究的过程。我们使用的预训练语料库有BooksCorpus（800M 个单词）和英语的维基百科（2,500M个单词）。维基百科我们仅使用了文本文章内容，忽略了列表、表格和标题。很重要的一点是，文章级的语料库比打乱的句子级语料库（如Billion Word Benchmark）要好，因为文章级能提取出长的连续序列。

##### 3.2 BERT微调

微调是一个简单的过程，因为转换器中的自注意机制使得BERT能够建模任何下游任务——不管是建模单一文本还是文本对的关系——只需要替换相应的输入和输出。在涉及到文本对的任务中，一个常见的处理方式是先分开各文本进行独立编码，再使用双向的交叉注意机制，如Parikh等人（2016）和Seo等人（2017）的研究工作。BERT则通过自注意机制将这两个阶段合并，直接编码合并的文本对，因此双向的交叉注意机制可以同时注意到两个句子的内容。

​		对于某个特定的任务，我们仅需要将该任务的输入输出契合进BERT，然后端对端地微调所有参数。再输入中，预训练中的句A和句B对应着

（1）同义句中的句子对；

（2）语义蕴含关系中的假设-前提对；

（3）问答任务中的问题-文章对；

（4）退化的文本-空值对，其中句A对应文本分类或者序列标注问题中的输入序列，句B为空。

在单词级任务（如序列标注和问答）中，每个词的表示后接输出层；而在句子级任务（如语义蕴含和情感分析）中，[CLS] 的表示后接输出层做分类。
		与预训练过程相比，微调的开销没有那么大。本文中所有的基于同一个预训练模型的微调训练结果可以在一个云TPU上最多训练1小时得到，或者在一个GPU上训练几小时。我们将在第四部分详细介绍各个任务模型的具体内容，更多细节可参考附录A.5。



#### 4 实验

这部分我们将展示BERT在11个NLP任务中的微调结果。

##### 4.1 GLUE

通用语言理解评估（GLUE）基准是一个多种自然语言理解任务的集合。GLUE数据集的详情可查看附录B.1。



#### 5 消融研究

#### 6 结论

#### 附录

附录包含三个部分：

* 附录A 展示了BERT模型实现的更多细节；
* 附录B 展示了我们实验的更多细节；
* 附录C 展示了更多的消融研究结果，包括：
  * 训练次数（steps）的影响；
  * 不同的遮盖替代概率的影响。

#### A  BERT模型的其他细节

##### A.1  预训练任务的说明

我们提供了预训练阶段的样本范例，如下所示。

**被遮盖的语言模型和遮盖过程**  假设未标注的句子是 my dog is hairy，而且在遮盖过程中我们随机选择了第4个单词（hairy）作为被遮盖的位置，则遮盖结果如下：

* 80% 的概率该词被[MASK]替代，如 my dog is hairy -> my dog is [MASK]
* 10% 的概率该词被替换为任意一个词，如 my dog is hairy -> my dog is apple
* 10% 的概率该词保持不变，如 my dog is hairy -> my dog is hairy。这样做是*<u>为了让该位置学到的表示尽量偏向实际观察到的词的表示</u>*。

​		选择这样替代的优点的是*<u>使得转换器的编码部分并不知道具体哪些词是预测对象，或者说不知道那些位置被随机的单词替换掉了，这样会迫使编码器针对输入的每个单词都学习其分布式上下文表示</u>*。另外，因为输入序列中仅有1.5%的词被替换为随机单词（15%的10%），所以替换并不会影响到模型的语言理解能力。附录C.2中比较了不同替换比率的影响。

​		相较于标准语言模型的训练过程而言，被遮盖的语言模型（MLM）在每一batch训练数据中只预测了15%的单词，因此可能需要训练更多次才能收敛。在附录C.1中我们展示了MLM的确比单向语言模型（预测了每个单词）收敛地稍微慢些，但是MLM在实际应用效果上的提升远远弥补了增加了训练成本。

**下一句预测**  下句预测任务样本样式可参考以下范例：

* 输入 =  [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]
* 标签 = IsNext

* 输入 =  [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]
* 标签 = NotNext

##### A.2  预训练过程

为了构造输入样本，我们从语料库中抽取了两段文本序列，我们称它们为“句子”，即使它们通常比一个真实的句子要长很多（当然也可能更短）。第一个句子作为A，第二个句子作为B。B有50%的概率是A的下一句, 50%的概率是从语料库中随机抽取的一个句子，用来预测B是否是A的下一句。抽取出来的两个句子合并后的序列长度应该 $\leq 512$。根据WordPiece进行分词后，会以15%的比率遮盖部分词，词汇表中每个词的被遮盖概率均一致，没有例外情况。

​		我们设定训练每批次（batch）为256个序列（256个序列/批次 * 512个单词/序列 = 128,000 单词/批次），共训练1,000,000次，因此大概在 33 亿单词量的语料库上训练 40 个回合（epoch）。我们使用学习率为1e-4，$\beta_1 = 0.9$，$\beta_2 = 0.999$的Adam优化器，L2权重衰减参数值为 0.01，学习率在前10,000次训练中热身，然后再设定其线性衰减。在每一层网络中，我们设置了0.1的神经元丢弃率（dropout）。参照OpenAI GPT，激活函数用的gelue而不是标准的relu。训练损失函数是MLM的平均似然与NSP的平均似然的和。

​		$BERT_{BASE}$ 的训练时在4个云挂载TPU（一共16块TPU）上进行。$BERT_{LARGE}$ 的训练在16个云挂载TPU（一共64块TPU）上进行。两个模型各花了4天的时间训练。

​		因为注意力机制的计算量二次于序列的长度，因此序列变长会导致训练成本非线性地大幅增长。为了加速预训练过程，我们90%的训练批次序列长度仅为128，剩下10%长度为512，用来学习位置编码参数。

##### A.3  微调过程

在微调中，大部分的超参数与预训练相同，除了批次大小，学习率和训练回合数。神经元丢弃率总是保持为0.1。根据任务的不同可选择不同的最优超参数，不过我们发现在所有任务中以下超参数范围表现得都不错：

* 批次大小：16，32
* Adam学习率：5e-5，3e-5，2e-5
* 训练回合数：2，3，4

​      我们同样发现大数据集（如十万以上标注训练样本）相对小数据集而言对超参数的选择更不敏感。微调过程是耗时很少，所以完全可以将以上训练参数组合都测试一遍，并且选择在验证集上变现最好的一组。

##### A.4  BERT，ELMo 与OpenAI GPT的比较

在这里我们将讨论下最近流行的三个表示学习模型：BERT，ELMo 与OpenAI GPT的不同点。三者模型结构的不同可见图3。要注意的是BERT 和 OpenAI GPT都是采用微调的方法，而ELMo是基于特征的。